{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo8cqH5AZtpw",
        "outputId": "37208cf1-865c-4b28-937e-439422599a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.42.0-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.5/263.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.42.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPqGqpf8ZyTJ",
        "outputId": "29927fc7-d45e-4cf2-ba21-c401ef092281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login 34c9a1875aa312280a5650afd5c45ab5d731d908"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "YTwbwB2PRtv5",
        "outputId": "efbbe913-d377-4a47-de74-d81ccfd732dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x500 with 10 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os  # Importing os module for operating system dependent functionality\n",
        "from keras.datasets import fashion_mnist  # Importing fashion_mnist dataset from keras\n",
        "import matplotlib.pyplot as plt  # Importing matplotlib for plotting\n",
        "import wandb  # Importing Weights & Biases library for experiment tracking\n",
        "\n",
        "\n",
        "# Loading Fashion MNIST dataset into training and testing sets\n",
        "(trainX,trainy),(testX,testy) = fashion_mnist.load_data()\n",
        "\n",
        "# Loading Fashion MNIST dataset into training and testing sets\n",
        "wandb.init(project='',entity='cs23m008',name='question 1')\n",
        "\n",
        "\n",
        "# List of classes in Fashion MNIST dataset\n",
        "class_list = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "i=0\n",
        "data_images=[]\n",
        "data_images_class=[]\n",
        "\n",
        "while(len(data_images_class) < 10):\n",
        "  if(class_list[trainy[i]] not in data_images_class):\n",
        "    data_images.append(trainX[i])\n",
        "    data_images_class.append(class_list[trainy[i]])\n",
        "  i+=1\n",
        "\n",
        "fig1 = plt.figure(figsize=(20,5))\n",
        "\n",
        "# Plotting sample images along with their corresponding classes\n",
        "for i in range(10):\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(data_images[i],cmap=plt.cm.binary)\n",
        "    plt.xlabel(data_images_class[i])\n",
        "    plt.grid(False)\n",
        "plt.show()\n",
        "wandb.log({\"Sample Image for each class\": fig1})\n",
        "wandb.save()\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkw3JSs2Er1A"
      },
      "source": [
        "# ACTIVATION FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AsUqQNWJEr1B"
      },
      "outputs": [],
      "source": [
        "# calculate different activation functions\n",
        "def sigmoid(x):\n",
        "  return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "\"\"\"\n",
        "  Function to compute the softmax activation function.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input data.\n",
        "\n",
        "    Returns:\n",
        "    - output: Softmax output.\n",
        "\"\"\"\n",
        "def softmax(x):\n",
        "  output = []\n",
        "  n = len(x)\n",
        "  for i in range(n):\n",
        "    total = 0\n",
        "    x[i] -= x[i][np.argmax(x[i])]\n",
        "    m = len(x[i])\n",
        "    for j in range(m):\n",
        "      total+=(np.exp(x[i][j]))\n",
        "    output.append(np.exp(x[i])/total)\n",
        "  return np.array(output)\n",
        "\n",
        "\"\"\"\n",
        "    Function to calculate cross-entropy loss.\n",
        "\n",
        "    Parameters:\n",
        "    - y_hat: Predicted labels.\n",
        "    - y: True labels.\n",
        "\n",
        "    Returns:\n",
        "    - error: Cross-entropy error.\n",
        "\"\"\"\n",
        "def cross_entropy(y_hat,y):\n",
        "  epsilon_ce = 1e-9\n",
        "  error = -(np.multiply(y,np.log(y_hat+epsilon_ce))).sum()/len(y_hat)\n",
        "  return error\n",
        "\n",
        "\"\"\"\n",
        "    Function to compute different activation functions.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input data.\n",
        "    - fn_label: Label for the activation function.\n",
        "\n",
        "    Returns:\n",
        "    - Activation output.\n",
        "\"\"\"\n",
        "def activation_functions(x,fn_label=\"sigmoid\") :\n",
        "  if fn_label == 'ReLU':\n",
        "    return relu(x)\n",
        "  elif fn_label == 'sigmoid' :\n",
        "    return sigmoid(x)\n",
        "  elif fn_label == 'tanh':\n",
        "    return tanh(x)\n",
        "  elif fn_label == 'softmax':\n",
        "    return softmax(x)\n",
        "  else:\n",
        "    return 'error'\n",
        "\n",
        "\"\"\"\n",
        "    Function to calculate mean squared error loss.\n",
        "\n",
        "    Parameters:\n",
        "    - y_cap: Predicted labels.\n",
        "    - y: True labels.\n",
        "\n",
        "    Returns:\n",
        "    - error: Mean squared error.\n",
        "\"\"\"\n",
        "def mean_squared_error(y_cap,y):\n",
        "  return np.sum(((y-y_cap)*(y-y_cap))/(len(y)+len(y)))\n",
        "\n",
        "\"\"\"\n",
        "    Function to compute derivatives of different activation functions.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input data.\n",
        "    - fn_label: Label for the activation function.\n",
        "\n",
        "    Returns:\n",
        "    - Derivative of the activation function.\n",
        "\"\"\"\n",
        "def activation_derivative(x, fn_label=\"sigmoid\"):\n",
        "    if fn_label == \"ReLU\":\n",
        "      return 1.0 * (x>0)\n",
        "    elif fn_label == \"tanh\":\n",
        "      return 1.0-(tanh(x)**2)\n",
        "    elif fn_label == \"sigmoid\":\n",
        "      return (1.0-sigmoid(x))*sigmoid(x)\n",
        "    else:\n",
        "      return 'error'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0rmchKtEr1C"
      },
      "source": [
        "# Question-2 Forward Propogation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIxvmQwcEr1D",
        "outputId": "7c1ce83b-a1f6-4041-e1fe-59409f5f0503"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 5yem77uw\n",
            "Sweep URL: https://wandb.ai/cs23m008/DL_Assignment_1/sweeps/5yem77uw\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "# ep = 10\n",
        "# bs = 30\n",
        "# lf = 'cross_entropy'\n",
        "# op = 'nadam'\n",
        "# lr = 1e-3\n",
        "data_set = 'fashion_mnist'\n",
        "\n",
        "m_beta = 0.9\n",
        "rmsprop_beta = 0.9\n",
        "beta_1 = 0.9\n",
        "beta_2 = 0.999\n",
        "epsilon_ = 1e-3\n",
        "wdc = 0\n",
        "# winit = 'Xavier'\n",
        "# nhls = 3\n",
        "# hls = 128\n",
        "# af = 'ReLU'\n",
        "# constants\n",
        "INPUT_KEY = 'input_size'\n",
        "OUTPUT_KEY = 'output_size'\n",
        "FUN_KEY = \"function\"\n",
        "\n",
        "def map_data_with_classes(classes):\n",
        "  rows = len(classes)\n",
        "  maxNum = 0\n",
        "  for i in range(rows):\n",
        "    if(maxNum<classes[i]) :\n",
        "      maxNum = classes[i]\n",
        "\n",
        "  cols = maxNum + 1\n",
        "  matrix = np.zeros((rows,cols))\n",
        "\n",
        "  for j in range(rows):\n",
        "    matrix[j][classes[j]] = 1\n",
        "  return matrix\n",
        "\n",
        "\n",
        "if data_set == 'fashion_mnist':\n",
        "  (train_X,train_Y),(test_X,test_Y) = fashion_mnist.load_data()\n",
        "elif data_set == 'mnist':\n",
        "  (train_X,train_Y),(test_X,test_Y) = mnist.load_data()\n",
        "\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "needed_y_train = train_Y\n",
        "needed_y_test = test_Y\n",
        "\n",
        "# splitting data into train and validation sets\n",
        "trainX, val_X, trainy, valy = train_test_split(train_X, train_Y, test_size=0.1, random_state=40)\n",
        "\n",
        "#flatten 2d image vectors to 1d vectors and treat them as training data\n",
        "trainX = trainX.reshape(len(trainX),len(trainX[0])*len(trainX[1]))\n",
        "testX = test_X.reshape(len(test_X),len(test_X[0])*len(test_X[1]))\n",
        "valX = val_X.reshape(len(val_X),len(val_X[0])*len(val_X[1]))\n",
        "\n",
        "trainX = trainX[:(len(trainX)//128)*128,:]\n",
        "testX = testX[:(len(testX)//128)*128,:]\n",
        "valX = valX[:(len(valX)//128)*128,:]\n",
        "\n",
        "trainy = trainy[:(len(trainy)//128)*128]\n",
        "test_Y = test_Y[:(len(test_Y)//128)*128]\n",
        "valy = valy[:(len(valy)//128)*128]\n",
        "\n",
        "# print(trainX.shape)\n",
        "# print(testX.shape)\n",
        "# print(valX.shape)\n",
        "# print(\"y\")\n",
        "# print(trainy.shape)\n",
        "# print(test_Y.shape)\n",
        "# print(valy.shape)\n",
        "\n",
        "# (54000, 784)\n",
        "# (10000, 784)\n",
        "# (6000, 784)\n",
        "\n",
        "trainy = map_data_with_classes(trainy)\n",
        "testy = map_data_with_classes(test_Y)\n",
        "valiy = map_data_with_classes(valy)\n",
        "\n",
        "input_layer_size = len(trainX[0])\n",
        "output_layer_size = len(trainy[0])\n",
        "\n",
        "# function to initialize the weights and biases\n",
        "def initialize_weights_and_biases(layers,number_hidden_layers = 1,init_type='random'):\n",
        "\n",
        "  \"\"\"\n",
        "    Function to initialize weights and biases for neural network layers.\n",
        "\n",
        "    Parameters:\n",
        "    - layers: List of dictionaries containing layer information (input and output sizes).\n",
        "    - number_hidden_layers: Number of hidden layers in the neural network (default is 1).\n",
        "    - init_type: Type of initialization for weights and biases ('random' or 'Xavier').\n",
        "\n",
        "    Returns:\n",
        "    - weights: List of weight matrices for each layer.\n",
        "    - biases: List of bias vectors for each layer.\n",
        "  \"\"\"\n",
        "  \n",
        "  weights ,biases = [],[]\n",
        "  OUTPUT_KEY = \"output_size\"\n",
        "  INPUT_KEY = \"input_size\"\n",
        "  if(init_type == 'random'):\n",
        "    for i in range(number_hidden_layers+1):\n",
        "      weights.append(np.random.normal(0,0.5,(layers[i][OUTPUT_KEY],layers[i][INPUT_KEY])))\n",
        "      biases.append(np.random.normal(0,0.5,(layers[i][OUTPUT_KEY],1)))\n",
        "  else:\n",
        "    # Xaviar\n",
        "    for i in range(number_hidden_layers+1):\n",
        "      right_x = np.sqrt(6/(layers[i][OUTPUT_KEY] + layers[i][INPUT_KEY]))\n",
        "      left_x = -1 * right_x\n",
        "      weights.append(np.random.uniform(left_x, right_x, size=(layers[i][OUTPUT_KEY], layers[i][INPUT_KEY])))\n",
        "      biases.append(np.random.uniform(left_x, right_x, size=(layers[i][OUTPUT_KEY], 1)))\n",
        "  return weights,biases\n",
        "\n",
        "\"\"\"\n",
        "    Function to calculate training accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    - batch_testy: True labels of the batch data.\n",
        "    - y_predicted: Predicted labels of the batch data.\n",
        "    - trainy: True labels of the entire training set.\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: Training accuracy.\n",
        "\"\"\"\n",
        "\n",
        "def train_accuracy(batch_testy,y_predicted,trainy):\n",
        "  c = 0\n",
        "  for j in range(len(batch_testy)):\n",
        "    for k in range(len(batch_testy[j])):\n",
        "      for l in range(len(batch_testy[j][k])):\n",
        "        if batch_testy[j][k][l] == 1 :\n",
        "          index_of_one = l\n",
        "          break\n",
        "      maxi = 0\n",
        "      for l in range(len(y_predicted[j][k])):\n",
        "        if y_predicted[j][k][l] > maxi :\n",
        "          maxi,pred_class = y_predicted[j][k][l],l\n",
        "      if(pred_class == index_of_one):\n",
        "        c+=1\n",
        "  return (c/len(trainy))\n",
        "\n",
        "\"\"\"\n",
        "    Function to calculate test accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    - testX: Input test data.\n",
        "    - testy: True labels of the test data.\n",
        "    - weights: List of weight matrices for each layer.\n",
        "    - biases: List of bias vectors for each layer.\n",
        "    - number_hidden_layers: Number of hidden layers in the neural network.\n",
        "    - activation_function: Activation function used in hidden layers.\n",
        "    - output_function: Output function used in the output layer.\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: Test accuracy.\n",
        "\"\"\"\n",
        "def test_accuracy(testX,testy,weights,biases,number_hidden_layers,activation_function,output_function):\n",
        "  a,h = forward_propagation(testX,weights,biases,number_hidden_layers, activation_function, output_function)\n",
        "  n_test = len(testy)\n",
        "  y_pred,y_predicted = h[-1],[]\n",
        "  n_pred = len(y_pred)\n",
        "  for i in range(n_pred):\n",
        "    y_predicted.append(np.argmax(y_pred[i]))\n",
        "  c = 0\n",
        "  for i in range(n_test):\n",
        "    if y_predicted[i] == testy[i]:\n",
        "      c+=1\n",
        "  return c/n_test\n",
        "\n",
        "\"\"\"\n",
        "    Function to calculate the regularization term.\n",
        "\n",
        "    Parameters:\n",
        "    - y: True labels of the data.\n",
        "    - weight_decay_const: Constant for weight decay.\n",
        "    - number_hidden_layers: Number of hidden layers in the neural network.\n",
        "    - weights: List of weight matrices for each layer.\n",
        "\n",
        "    Returns:\n",
        "    - reg_term: Regularization term.\n",
        "\"\"\"\n",
        "def calculate_regularizing_term(y,weight_decay_const,number_hidden_layers,weights):\n",
        "  reg_term = 0.0\n",
        "  for i in range(number_hidden_layers+1):\n",
        "    reg_term += (np.sum(weights[i]*weights[i]))\n",
        "  n_y = len(y)\n",
        "  temp = weight_decay_const/(2*n_y)\n",
        "  reg_term = ((temp)*(reg_term))\n",
        "  return reg_term\n",
        "\n",
        "\"\"\"\n",
        "    Function to calculate validation loss.\n",
        "\n",
        "    Parameters:\n",
        "    - valX: Input validation data.\n",
        "    - valy: True labels of the validation data.\n",
        "    - weights: List of weight matrices for each layer.\n",
        "    - biases: List of bias vectors for each layer.\n",
        "    - number_hidden_layers: Number of hidden layers in the neural network.\n",
        "    - activation_function: Activation function used in hidden layers.\n",
        "    - output_function: Output function used in the output layer.\n",
        "    - loss_function: Loss function used to calculate the error.\n",
        "\n",
        "    Returns:\n",
        "    - error: Validation loss.\n",
        "\"\"\"\n",
        "def val_loss(valX,valy,weights,biases,number_hidden_layers, activation_function,output_function,loss_function):\n",
        "  a,h = forward_propagation(valX,weights,biases,number_hidden_layers, activation_function, output_function)\n",
        "  y_hat = h[-1]\n",
        "  if loss_function == 'cross_entropy':\n",
        "    error = cross_entropy(y_hat,valy)\n",
        "  elif loss_function == 'mean_squared_error':\n",
        "    error = mean_squared_error(y_hat,valy)\n",
        "  return error\n",
        "\n",
        "\n",
        " \"\"\"\n",
        "    Function to perform forward propagation in a neural network.\n",
        "\n",
        "    Parameters:\n",
        "    - input_x: Input data.\n",
        "    - W: List of weight matrices for each layer.\n",
        "    - B: List of bias vectors for each layer.\n",
        "    - hidden_layers: Number of hidden layers in the neural network.\n",
        "    - activ_label: Label for activation function.\n",
        "    - op_label: Label for output function.\n",
        "\n",
        "    Returns:\n",
        "    - a: List of activation matrices for each layer.\n",
        "    - h: List of hidden state matrices for each layer.\n",
        " \"\"\"\n",
        "def forward_propagation(input_x, W, B, hidden_layers, activ_label, op_label):\n",
        "  # Initialize lists to store activations (a) and hidden states (h)\n",
        "  a, h = [], []\n",
        "\n",
        "  # Reshape input_x if necessary\n",
        "  n, m = len(input_x), len(input_x[0])\n",
        "  batch_trainX = np.reshape(input_x, (n, m))\n",
        "\n",
        "  # Compute activations and hidden states for the first hidden layer\n",
        "  a.append(np.dot(W[0], batch_trainX.T) + B[0])\n",
        "  h.append(activation_functions(np.dot(W[0],batch_trainX.T) + B[0],activ_label))\n",
        "\n",
        "  # Compute activations and hidden states for subsequent hidden layers\n",
        "  for i in range(1, hidden_layers):\n",
        "    an = np.dot(W[i], h[i-1]) + B[i]\n",
        "    a.append(an)\n",
        "    hn = activation_functions(an, activ_label)\n",
        "    h.append(hn)\n",
        "\n",
        "  # Compute activations and hidden states for the output layer\n",
        "  aL = np.matmul(W[hidden_layers], h[hidden_layers-1]) + B[hidden_layers]\n",
        "  hL = activation_functions(aL.T, op_label)\n",
        "  hL = hL.T\n",
        "  a.append(aL)\n",
        "  h.append(hL)\n",
        "\n",
        "  # Transpose activation and hidden state lists for consistency\n",
        "  for i in range(0, hidden_layers+1):\n",
        "    a[i], h[i] = a[i].T, h[i].T\n",
        "\n",
        "  return a, h\n",
        "\n",
        "def backward_propagation(batch_trainy , batch_trainX ,y_hat , a, h, weights, number_hidden_layers ,derivative_function = 'sigmoid'):\n",
        "  del_a,del_W,del_b,del_h = {},{},{},{}\n",
        "\n",
        "  batch_trainy = batch_trainy.reshape(len(batch_trainy),len(batch_trainy[0]))\n",
        "\n",
        "  ep =1e-8\n",
        "  keyA = 'a'+ str(number_hidden_layers+1)\n",
        "  keyH ='h'+ str(number_hidden_layers+1)\n",
        "  del_a[keyA] = -(batch_trainy-y_hat)\n",
        "  del_h[keyH] = -(batch_trainy/(y_hat+ep))\n",
        "  xval = len(batch_trainX)\n",
        "  # starting from last layer and going to first layer\n",
        "  for i in range(number_hidden_layers + 1,1,-1):\n",
        "    # calculating del of weights from del of a and h from foward propagation\n",
        "    keyW = 'W' + str(i)\n",
        "    keyB = 'b' + str(i)\n",
        "    keyA = 'a'+ str(i)\n",
        "    prevKeyA = 'a'+str(i-1)\n",
        "    prevKeyH = 'h'+str(i-1)\n",
        "    del_W[keyW] = np.dot(del_a[keyA].T,h[i-2])\n",
        "\n",
        "    # applying L2 regularization\n",
        "    del_W[keyW] += (wdc * weights[i-1])\n",
        "    del_W[keyW]/=xval\n",
        "\n",
        "    # calculating del of biases from del of a\n",
        "    del_b[keyB] = del_a[keyA]\n",
        "\n",
        "    # calculating del of h from weights and del of a\n",
        "    del_h[prevKeyH] = np.dot(weights[i-1].T , del_a[keyA].T)\n",
        "\n",
        "    # calculating del of a from del of h\n",
        "    del_a[prevKeyA] = np.multiply(del_h[prevKeyH],activation_derivative(a[i-2].T,derivative_function))\n",
        "    del_a[prevKeyA] = del_a[prevKeyA].T\n",
        "\n",
        "\n",
        "  # for first we only need to calculate del W and del b and not del h and del a\n",
        "  del_W['W'+str('1')],del_b['b'+str('1')] = np.dot(del_a['a1'].T,batch_trainX),del_a['a1']\n",
        "\n",
        "  for j in range(1,len(del_b)+1):\n",
        "    li = []\n",
        "    k = 0\n",
        "    while k<len(del_b['b'+str(j)][0]):\n",
        "      sum = 0\n",
        "      l = 0\n",
        "      while l<len(del_b['b'+str(j)]):\n",
        "        sum += del_b['b'+str(j)][l][k]\n",
        "        l+=1\n",
        "      li.append(sum/xval)\n",
        "      k+=1\n",
        "    li = np.array(li)\n",
        "    del_b['b'+str(j)] = li.reshape(len(li),1)\n",
        "\n",
        "  return del_W,del_b\n",
        "\n",
        "def gradient_descent(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):\n",
        "  layers = []\n",
        "  layers.append({INPUT_KEY : input_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "  for i in range(number_hidden_layers-1):\n",
        "    layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "  layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : output_layer_size, FUN_KEY : output_function})\n",
        "\n",
        "#initialize weights and biases\n",
        "  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)\n",
        "  x_val = len(trainX)\n",
        "  number_batches = x_val/mini_batch_size\n",
        "\n",
        "  mini_batch_trainX,mini_batch_trainy = np.array(np.array_split(trainX, number_batches)),np.array(np.array_split(trainy, number_batches))\n",
        "\n",
        "  train_loss_list,  val_loss_list,  train_acc_list,  val_acc_list = [],[],[],[]\n",
        "  h=None\n",
        "  for j in range(epochs):\n",
        "    tloss,vloss = 0,0\n",
        "    y_predicted = []\n",
        "    for k in range(len(mini_batch_trainX)):\n",
        "      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)\n",
        "      y_predicted.append(h[-1])\n",
        "\n",
        "      if loss_function == 'cross_entropy':\n",
        "        tloss += cross_entropy(h[-1],mini_batch_trainy[k])\n",
        "      elif loss_function == 'mean_squared_error':\n",
        "        tloss += mean_squared_error( h[-1],mini_batch_trainy[k])\n",
        "      else:\n",
        "        print('wrong loss function')\n",
        "\n",
        "      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)\n",
        "\n",
        "      for i in range(len(weights)):\n",
        "        keyW = 'W'+str(i+1)\n",
        "        keyB = 'b'+str(i+1)\n",
        "        weights[i] -=  (del_W[keyW]*eta)\n",
        "        biases[i] -= (del_b[keyB]*eta)\n",
        "    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)\n",
        "    tr_loss = tloss/number_batches + reg_term_train\n",
        "    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)\n",
        "    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)\n",
        "    vloss = vloss + reg_term_val\n",
        "    print(\"epoch : \",j+1,\" validation loss : \",vloss)\n",
        "\n",
        "    train_loss_list.append(tr_loss)\n",
        "    val_loss_list.append(vloss)\n",
        "    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    if wandb_flag == True:\n",
        "      wandb.log({\"loss\":tr_loss,\"val_loss\":vloss,\"accuracy\":train_acc,\"val_accuracy\":val_acc,\"epoch\":j})\n",
        "\n",
        "  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]\n",
        "  return h[-1],weights,biases,plot_lists\n",
        "\n",
        "\n",
        "\n",
        "def momentum_based_gradient_descent(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):\n",
        "\n",
        "\n",
        "\n",
        "  #initialize layers of neural networks\n",
        "  layers = []\n",
        "  inputLayer = {INPUT_KEY : input_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function}\n",
        "  layers.append(inputLayer)\n",
        "\n",
        "  for i in range(number_hidden_layers-1):\n",
        "    hiddenLayer = {INPUT_KEY : hidden_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function}\n",
        "    layers.append(hiddenLayer)\n",
        "\n",
        "  outputLayer = {INPUT_KEY : hidden_layer_size, OUTPUT_KEY : output_layer_size, FUN_KEY : output_function}\n",
        "  layers.append(outputLayer)\n",
        "\n",
        "  #initialize weights and biases\n",
        "\n",
        "  number_batches = len(trainX)/mini_batch_size\n",
        "\n",
        "  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)\n",
        "  mini_batch_trainX,mini_batch_trainy = np.array(np.array_split(trainX, number_batches)),np.array(np.array_split(trainy, number_batches))\n",
        "\n",
        "  past_weights,past_biases = [],[]\n",
        "\n",
        "  for i in range(number_hidden_layers+1):\n",
        "    mW,mB,nW,nB = len(weights[i]),len(biases[i]),len(weights[i][0]),len(biases[i][0])\n",
        "    past_weights.append(np.zeros((mW,nW)))\n",
        "    past_biases.append(np.zeros((mB,nB)))\n",
        "\n",
        "  train_loss_list,  val_loss_list,  train_acc_list,  val_acc_list = [],[],[],[]\n",
        "  h=None\n",
        "  for j in range(epochs):\n",
        "    tloss = 0\n",
        "    vloss = 0\n",
        "    y_predicted = []\n",
        "    for k in range(len(mini_batch_trainX)):\n",
        "      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)\n",
        "      y_predicted.append(h[-1])\n",
        "\n",
        "      if loss_function == 'cross_entropy':\n",
        "        tloss += cross_entropy(h[-1],mini_batch_trainy[k])\n",
        "      elif loss_function == 'mean_squared_error':\n",
        "        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])\n",
        "      else:\n",
        "        print('wrong loss function')\n",
        "\n",
        "      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)\n",
        "\n",
        "      for i in range(number_hidden_layers+1):\n",
        "        keyW = 'W' + str(i+1)\n",
        "        keyB = 'b' + str(i+1)\n",
        "        past_weights[i],past_biases[i] = (past_weights[i]*m_beta) + (del_W[keyW] * eta),(past_biases[i]*m_beta) + (del_b[keyB] * eta)\n",
        "\n",
        "        weights[i],biases[i] = weights[i]-past_weights[i],biases[i]-past_biases[i]\n",
        "\n",
        "    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)\n",
        "    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)\n",
        "    tr_loss = tloss/number_batches + reg_term_train\n",
        "    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)\n",
        "    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)\n",
        "    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)\n",
        "    vloss = vloss + reg_term_val\n",
        "    print(\"epoch : \",j+1,\" validation loss : \",vloss)\n",
        "\n",
        "    train_loss_list.append(tr_loss)\n",
        "    val_loss_list.append(vloss)\n",
        "    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    if wandb_flag == True:\n",
        "      wandb.log({\"loss\":tr_loss,\"val_loss\":vloss,\"accuracy\":train_acc,\"val_accuracy\":val_acc,\"epoch\":j})\n",
        "\n",
        "  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]\n",
        "  return h[-1],weights,biases,plot_lists\n",
        "\n",
        "def nestrov_accelerated_gradient_descent(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):\n",
        "\n",
        "  layers = []\n",
        "  layers.append({INPUT_KEY : input_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "\n",
        "  for i in range(number_hidden_layers-1):\n",
        "    layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "\n",
        "  layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : output_layer_size, FUN_KEY : output_function})\n",
        "\n",
        "  #initialize weights and biases\n",
        "\n",
        "  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)\n",
        "  x_val = len(trainX)\n",
        "  number_batches = x_val/mini_batch_size\n",
        "\n",
        "  mini_batch_trainX,mini_batch_trainy = np.array(np.array_split(trainX, number_batches)),np.array(np.array_split(trainy, number_batches))\n",
        "\n",
        "  past_weights,past_biases = [],[]\n",
        "\n",
        "  # beta = 0.9\n",
        "\n",
        "  for i in range(number_hidden_layers+1):\n",
        "    past_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))\n",
        "    past_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))\n",
        "\n",
        "  train_loss_list,  val_loss_list,  train_acc_list,  val_acc_list, = [],[],[],[]\n",
        "  h=None\n",
        "  for j in range(epochs):\n",
        "    tloss,vloss = 0,0\n",
        "    y_predicted = []\n",
        "    for k in range(len(mini_batch_trainX)):\n",
        "      lookahead_weights,lookahead_biases = [],[]\n",
        "      for l in range(number_hidden_layers+1):\n",
        "        lookahead_weights.append(weights[l] - (past_weights[l] * m_beta))\n",
        "        lookahead_biases.append(biases[l] - (past_biases[l] * m_beta))\n",
        "      a,h = forward_propagation(mini_batch_trainX[k],lookahead_weights,lookahead_biases,number_hidden_layers, activation_function, output_function)\n",
        "      y_predicted.append(h[-1])\n",
        "\n",
        "      if loss_function == 'cross_entropy':\n",
        "        tloss += cross_entropy(h[-1],mini_batch_trainy[k])\n",
        "      elif loss_function == 'mean_squared_error':\n",
        "        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])\n",
        "      else:\n",
        "        print('wrong loss function')\n",
        "\n",
        "      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,lookahead_weights,number_hidden_layers ,activation_function)\n",
        "\n",
        "      for i in range(number_hidden_layers+1):\n",
        "        keyW = 'W' + str(i+1)\n",
        "        keyB = 'b' + str(i+1)\n",
        "        past_weights[i] = (past_weights[i]*m_beta) + (del_W[keyW] * eta)\n",
        "        past_biases[i] = (past_biases[i]*m_beta) + (del_b[keyB] * eta)\n",
        "\n",
        "        weights[i],biases[i] = weights[i]-past_weights[i], biases[i]-past_biases[i]\n",
        "\n",
        "    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)\n",
        "    tr_loss = tloss/number_batches + reg_term_train\n",
        "    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)\n",
        "    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)\n",
        "    vloss = vloss + reg_term_val\n",
        "    print(\"epoch : \",j+1,\" validation loss : \",vloss)\n",
        "\n",
        "    train_loss_list.append(tr_loss)\n",
        "    val_loss_list.append(vloss)\n",
        "    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    if wandb_flag == True:\n",
        "      wandb.log({\"loss\":tr_loss,\"val_loss\":vloss,\"accuracy\":train_acc,\"val_accuracy\":val_acc,\"epoch\":j})\n",
        "\n",
        "  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]\n",
        "  return h[-1],weights,biases,plot_lists\n",
        "\n",
        "def rmsprop(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):\n",
        "\n",
        "  layers = []\n",
        "  layers.append({INPUT_KEY : input_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "\n",
        "  for i in range(number_hidden_layers-1):\n",
        "    layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "\n",
        "  layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : output_layer_size, FUN_KEY : output_function})\n",
        "\n",
        "  #initialize weights and biases\n",
        "\n",
        "  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)\n",
        "  x_val = len(trainX)\n",
        "  number_batches = x_val/mini_batch_size\n",
        "\n",
        "  mini_batch_trainX,mini_batch_trainy = np.array(np.array_split(trainX, number_batches)),np.array(np.array_split(trainy, number_batches))\n",
        "\n",
        "  v_weights,v_biases = [],[]\n",
        "\n",
        "  # beta = 0.9\n",
        "  # ep = 1e-3\n",
        "\n",
        "  for i in range(number_hidden_layers+1):\n",
        "    v_weights.append(np.zeros((len(weights[i]),len(weights[i][0]))))\n",
        "    v_biases.append(np.zeros((len(biases[i]),len(biases[i][0]))))\n",
        "\n",
        "  train_loss_list,  val_loss_list,  train_acc_list,  val_acc_list, = [],[],[],[]\n",
        "  h=None\n",
        "  for j in range(epochs):\n",
        "    tloss,vloss = 0,0\n",
        "    y_predicted = []\n",
        "    for k in range(len(mini_batch_trainX)):\n",
        "      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)\n",
        "      y_predicted.append(h[-1])\n",
        "\n",
        "      if loss_function == 'cross_entropy':\n",
        "        tloss += cross_entropy(h[-1],mini_batch_trainy[k])\n",
        "      elif loss_function == 'mean_squared_error':\n",
        "        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])\n",
        "      else:\n",
        "        print('wrong loss function')\n",
        "\n",
        "      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)\n",
        "\n",
        "      for i in range(number_hidden_layers+1):\n",
        "        keyW = 'W' + str(i+1)\n",
        "        keyB = 'b' + str(i+1)\n",
        "        v_weights[i] = (v_weights[i]*rmsprop_beta) + ((del_W[keyW]**2) * (1-rmsprop_beta))\n",
        "        v_biases[i] = (v_biases[i]*rmsprop_beta) + ((del_b[keyB]**2) * (1-rmsprop_beta))\n",
        "\n",
        "        v_weight_prime = v_weights[i] + epsilon_\n",
        "        weights[i] -= (((del_W[keyW]/np.sqrt(v_weight_prime)))*eta)\n",
        "        v_bias_prime = v_biases[i] + epsilon_\n",
        "        biases[i] -= (((del_b[keyB]/np.sqrt(v_bias_prime)))*eta)\n",
        "\n",
        "    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)\n",
        "    tr_loss = tloss/number_batches + reg_term_train\n",
        "    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)\n",
        "    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)\n",
        "    vloss = vloss + reg_term_val\n",
        "    print(\"epoch : \",j+1,\" validation loss : \",vloss)\n",
        "\n",
        "    train_loss_list.append(tr_loss)\n",
        "    val_loss_list.append(vloss)\n",
        "    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    if wandb_flag == True:\n",
        "      wandb.log({\"loss\":tr_loss,\"val_loss\":vloss,\"accuracy\":train_acc,\"val_accuracy\":val_acc,\"epoch\":j})\n",
        "\n",
        "  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]\n",
        "  return h[-1],weights,biases,plot_lists\n",
        "def adam(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const=0,wandb_flag=False):\n",
        "\n",
        "\n",
        "  layers = []\n",
        "  layers.append({INPUT_KEY : input_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "\n",
        "  for i in range(number_hidden_layers-1):\n",
        "    layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "\n",
        "  layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : output_layer_size, FUN_KEY : output_function})\n",
        "\n",
        "  #initialize weights and biases\n",
        "\n",
        "  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)\n",
        "  x_val = len(trainX)\n",
        "  number_batches = x_val/mini_batch_size\n",
        "\n",
        "  mini_batch_trainX,mini_batch_trainy = np.array(np.array_split(trainX, number_batches)),np.array(np.array_split(trainy, number_batches))\n",
        "\n",
        "  # beta1 = 0.9\n",
        "  # beta2 = 0.999\n",
        "  # ep = 1e-3\n",
        "\n",
        "  v_weights,v_biases,v_hat_weights,v_hat_biases,m_weights,m_biases,m_hat_weights,m_hat_biases = [],[],[],[],[],[],[],[]\n",
        "\n",
        "  for i in range(number_hidden_layers+1):\n",
        "    weight_val_m = len(weights[i])\n",
        "    weight_val_n = len(weights[i][0])\n",
        "\n",
        "    bias_val_m = len(biases[i])\n",
        "    bias_val_n = len(biases[i][0])\n",
        "    v_weights.append(np.zeros((weight_val_m,weight_val_n)))\n",
        "    v_biases.append(np.zeros((bias_val_m,bias_val_n)))\n",
        "    v_hat_weights.append(np.zeros((weight_val_m,weight_val_n)))\n",
        "    v_hat_biases.append(np.zeros((bias_val_m,bias_val_n)))\n",
        "    m_weights.append(np.zeros((weight_val_m,weight_val_n)))\n",
        "    m_biases.append(np.zeros((bias_val_m,bias_val_n)))\n",
        "    m_hat_weights.append(np.zeros((weight_val_m,weight_val_n)))\n",
        "    m_hat_biases.append(np.zeros((bias_val_m,bias_val_n)))\n",
        "\n",
        "  train_loss_list,  val_loss_list,  train_acc_list,  val_acc_list, = [],[],[],[]\n",
        "  h=None\n",
        "  c = 0\n",
        "  for j in range(epochs):\n",
        "    tloss = 0\n",
        "    vloss = 0\n",
        "    y_predicted = []\n",
        "    for k in range(len(mini_batch_trainX)):\n",
        "      c+=1\n",
        "      a,h = forward_propagation(mini_batch_trainX[k],weights,biases,number_hidden_layers, activation_function, output_function)\n",
        "      y_predicted.append(h[-1])\n",
        "\n",
        "      if loss_function == 'cross_entropy':\n",
        "        tloss += cross_entropy(h[-1],mini_batch_trainy[k])\n",
        "      elif loss_function == 'mean_squared_error':\n",
        "        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])\n",
        "      else:\n",
        "        print('wrong loss function')\n",
        "\n",
        "      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,weights,number_hidden_layers ,activation_function)\n",
        "\n",
        "      for i in range(len(weights)):\n",
        "        keyW = 'W'+str(i+1)\n",
        "        keyB = 'b'+str(i+1)\n",
        "        v_weights[i] = (v_weights[i]*beta_2) + (((del_W[keyW])*(del_W[keyW]))*(1-beta_2))\n",
        "        v_biases[i] = (v_biases[i]*beta_2) + (((del_b[keyB])*(del_b[keyB]))*(1-beta_2))\n",
        "\n",
        "        m_weights[i] = (m_weights[i]*beta_1) + (del_W[keyW]*(1-beta_1))\n",
        "        m_biases[i] = (m_biases[i]*beta_1) + (del_b[keyB]*(1-beta_1))\n",
        "\n",
        "        temp1 = (1-beta_1**c)\n",
        "        temp2 = 1-beta_2**c\n",
        "        v_hat_weights[i] = (v_weights[i]/temp2)\n",
        "        v_hat_biases[i] = (v_biases[i]/temp2)\n",
        "\n",
        "        m_hat_weights[i] = (m_weights[i]/temp1)\n",
        "        m_hat_biases[i] = (m_biases[i]/temp1)\n",
        "\n",
        "        weights[i] -= ((m_hat_weights[i]*eta/np.sqrt(v_hat_weights[i] + epsilon_)))\n",
        "        biases[i] -= ((m_hat_biases[i]*eta/np.sqrt(v_hat_biases[i] + epsilon_)))\n",
        "\n",
        "    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)\n",
        "    tr_loss = tloss/number_batches + reg_term_train\n",
        "    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)\n",
        "    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)\n",
        "    vloss = vloss + reg_term_val\n",
        "    print(\"epoch : \",j+1,\" validation loss : \",vloss)\n",
        "\n",
        "    train_loss_list.append(tr_loss)\n",
        "    val_loss_list.append(vloss)\n",
        "    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)\n",
        "    val_acc_list.append(val_acc)\n",
        "\n",
        "    if wandb_flag == True:\n",
        "      wandb.log({\"loss\":tr_loss,\"val_loss\":vloss,\"accuracy\":train_acc,\"val_accuracy\":val_acc,\"epoch\":j})\n",
        "\n",
        "  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]\n",
        "  return h[-1],weights,biases,plot_lists\n",
        "\n",
        "def nadam(trainX, trainy, number_hidden_layers = 1, hidden_layer_size = 4, eta = 0.1, initial_weights = 'random', activation_function = 'sigmoid', epochs = 1, output_function = 'softmax', mini_batch_size=4,loss_function = 'cross_entropy',weight_decay_const = 0,wandb_flag=False):\n",
        "\n",
        "#initialize layers of neural networks\n",
        "\n",
        "\n",
        "  layers = []\n",
        "  layers.append({INPUT_KEY : input_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "\n",
        "  for i in range(number_hidden_layers-1):\n",
        "    layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : hidden_layer_size, FUN_KEY : activation_function})\n",
        "\n",
        "  layers.append({INPUT_KEY : hidden_layer_size, OUTPUT_KEY : output_layer_size, FUN_KEY : output_function})\n",
        "\n",
        "  #initialize weights and biases\n",
        "\n",
        "  weights,biases = initialize_weights_and_biases(layers,number_hidden_layers,initial_weights)\n",
        "  x_val = len(trainX)\n",
        "  number_batches = x_val/mini_batch_size\n",
        "\n",
        "  mini_batch_trainX,mini_batch_trainy = np.array(np.array_split(trainX, number_batches)),np.array(np.array_split(trainy, number_batches))\n",
        "\n",
        "  # beta1 = 0.9\n",
        "  # beta2 = 0.999\n",
        "  # ep = 1e-3\n",
        "\n",
        "  v_weights,v_biases,v_hat_weights,v_hat_biases,m_weights,m_biases,m_hat_weights,m_hat_biases = [],[],[],[],[],[],[],[]\n",
        "\n",
        "  for i in range(number_hidden_layers+1):\n",
        "    weight_val_m = len(weights[i])\n",
        "    weight_val_n = len(weights[i][0])\n",
        "\n",
        "    bias_val_m = len(biases[i])\n",
        "    bias_val_n = len(biases[i][0])\n",
        "    v_weights.append(np.zeros((weight_val_m,weight_val_n)))\n",
        "    v_biases.append(np.zeros((bias_val_m,bias_val_n)))\n",
        "    v_hat_weights.append(np.zeros((weight_val_m,weight_val_n)))\n",
        "    v_hat_biases.append(np.zeros((bias_val_m,bias_val_n)))\n",
        "    m_weights.append(np.zeros((weight_val_m,weight_val_n)))\n",
        "    m_biases.append(np.zeros((bias_val_m,bias_val_n)))\n",
        "    m_hat_weights.append(np.zeros((weight_val_m,weight_val_n)))\n",
        "    m_hat_biases.append(np.zeros((bias_val_m,bias_val_n)))\n",
        "\n",
        "  train_loss_list,  val_loss_list,  train_acc_list,  val_acc_list, = [],[],[],[]\n",
        "  h=None\n",
        "  c = 0\n",
        "  for j in range(epochs):\n",
        "    tloss = 0\n",
        "    vloss = 0\n",
        "    y_predicted = []\n",
        "    for k in range(len(mini_batch_trainX)):\n",
        "      lookahead_weights,lookahead_biases,lookahead_v_hat_weights,lookahead_v_hat_biases,lookahead_m_hat_weights,lookahead_m_hat_biases = [],[],[],[],[],[]\n",
        "      c+=1\n",
        "      for l in range(number_hidden_layers+1):\n",
        "        temp2 = (1 - (beta_2**c))\n",
        "        temp1 = (1 - beta_1**c)\n",
        "        lookahead_v_hat_weights.append((v_weights[l]*beta_2)/temp2)\n",
        "        lookahead_v_hat_biases.append((v_biases[l]*beta_2)/temp2)\n",
        "\n",
        "        lookahead_m_hat_weights.append((m_weights[l]*beta_1)/temp1)\n",
        "        lookahead_m_hat_biases.append((m_biases[l]*beta_1)/ temp1)\n",
        "\n",
        "        w_hat_temp = lookahead_v_hat_weights[l] + epsilon_\n",
        "        b_hat_temp = lookahead_v_hat_biases[l] + epsilon_\n",
        "        lookahead_weights.append(weights[l] - (lookahead_m_hat_weights[l] / np.sqrt(w_hat_temp))*eta)\n",
        "        lookahead_biases.append(biases[l] - (lookahead_m_hat_biases[l] / np.sqrt(b_hat_temp))*eta)\n",
        "\n",
        "      a,h = forward_propagation(mini_batch_trainX[k],lookahead_weights,lookahead_biases,number_hidden_layers, activation_function, output_function)\n",
        "      y_predicted.append(h[-1])\n",
        "\n",
        "      if loss_function == 'cross_entropy':\n",
        "        tloss += cross_entropy(h[-1],mini_batch_trainy[k])\n",
        "      elif loss_function == 'mean_squared_error':\n",
        "        tloss += mean_squared_error(h[-1],mini_batch_trainy[k])\n",
        "      else:\n",
        "        print('wrong loss function')\n",
        "\n",
        "      del_W,del_b = backward_propagation(mini_batch_trainy[k],mini_batch_trainX[k],h[-1],a,h,lookahead_weights,number_hidden_layers ,activation_function)\n",
        "\n",
        "      for i in range(len(weights)):\n",
        "        keyW = 'W'+str(i+1)\n",
        "        keyB = 'b'+str(i+1)\n",
        "\n",
        "        v_weights[i] = (v_weights[i]*beta_2) + ((del_W[keyW]*del_W[keyW])*(1-beta_2))\n",
        "        v_biases[i] = (v_biases[i]*beta_2) + (del_b[keyB]*(del_b[keyB])*(1-beta_2))\n",
        "\n",
        "        m_weights[i] = (m_weights[i]*beta_1) + (del_W[keyW]*(1-beta_1))\n",
        "        m_biases[i] = (m_biases[i]*beta_1) + (del_b[keyB]*(1-beta_1))\n",
        "\n",
        "        temp1 = (1-beta_1**c)\n",
        "        temp2 = (1-beta_2**c)\n",
        "        v_hat_weights[i] = (v_weights[i]/temp2)\n",
        "        v_hat_biases[i] = (v_biases[i]/temp2)\n",
        "\n",
        "        m_hat_weights[i] = (m_weights[i]/temp1)\n",
        "        m_hat_biases[i] = (m_biases[i]/temp1)\n",
        "\n",
        "        w_hat_temp = v_hat_weights[i] + epsilon_\n",
        "        b_hat_temp = v_hat_biases[i] + epsilon_\n",
        "        weights[i] = weights[i] - ((m_hat_weights[i]*eta/np.sqrt(w_hat_temp)))\n",
        "        biases[i] = biases[i] - ((m_hat_biases[i]*eta/np.sqrt(b_hat_temp)))\n",
        "\n",
        "    reg_term_train = calculate_regularizing_term(trainy,weight_decay_const,number_hidden_layers,weights)\n",
        "    tr_loss = tloss/number_batches + reg_term_train\n",
        "    vloss = val_loss(valX,valiy,weights,biases,number_hidden_layers,activation_function,output_function,loss_function)\n",
        "    reg_term_val = calculate_regularizing_term(valiy,weight_decay_const,number_hidden_layers,weights)\n",
        "    vloss = vloss + reg_term_val\n",
        "    print(\"epoch : \",j+1,\" validation loss : \",vloss)\n",
        "\n",
        "    train_loss_list.append(tr_loss)\n",
        "    val_loss_list.append(vloss)\n",
        "    train_acc = train_accuracy(mini_batch_trainy,y_predicted,trainy)\n",
        "    train_acc_list.append(train_acc)\n",
        "    val_acc = test_accuracy(valX,valy,weights,biases,number_hidden_layers,activation_function,output_function)\n",
        "    val_acc_list.append(val_acc)\n",
        "    if wandb_flag == True:\n",
        "      wandb.log({\"loss\":tr_loss,\"val_loss\":vloss,\"accuracy\":train_acc,\"val_accuracy\":val_acc,\"epoch\":j})\n",
        "\n",
        "  plot_lists = [train_loss_list,val_loss_list,train_acc_list,val_acc_list]\n",
        "  return h[-1],weights,biases,plot_lists\n",
        "\n",
        "\n",
        "\n",
        "def train(trainX,trainy,textX,testy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,mini_batch_size,loss_function,optimizer,output_function,weight_decay_const,wandb_flag=False):\n",
        "  wdc = weight_decay_const\n",
        "  if optimizer=='sgd':\n",
        "    hL,weights,biases,plot_list = gradient_descent(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)\n",
        "    params = [weights,biases,number_hidden_layers,activation_function,output_function]\n",
        "    return params\n",
        "  elif optimizer=='momentum':\n",
        "    hL,weights,biases,plot_list = momentum_based_gradient_descent(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)\n",
        "    params = [weights,biases,number_hidden_layers,activation_function,output_function]\n",
        "    return params\n",
        "\n",
        "  elif optimizer=='nag':\n",
        "    hL,weights,biases,plot_list = nestrov_accelerated_gradient_descent(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)\n",
        "    params = [weights,biases,number_hidden_layers,activation_function,output_function]\n",
        "    return params\n",
        "  elif optimizer=='rmsprop':\n",
        "    hL,weights,biases,plot_list = rmsprop(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)\n",
        "    params = [weights,biases,number_hidden_layers,activation_function,output_function]\n",
        "    return params\n",
        "\n",
        "  elif optimizer == 'adam':\n",
        "\n",
        "    hL,weights,biases,plot_list = adam(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)\n",
        "    params = [weights,biases,number_hidden_layers,activation_function,output_function]\n",
        "\n",
        "    return params\n",
        "\n",
        "  elif optimizer == 'nadam':\n",
        "    hL,weights,biases,plot_list = nadam(trainX,trainy,number_hidden_layers,hidden_layer_size,eta,init_type,activation_function,epochs,output_function,mini_batch_size,loss_function,weight_decay_const,wandb_flag)\n",
        "    params = [weights,biases,number_hidden_layers,activation_function,output_function]\n",
        "    return params\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"val_accuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "    \"e\": {\n",
        "        \"values\": [5, 10]\n",
        "    },\n",
        "\n",
        "    \"w_i\": {\n",
        "        \"values\": [\"random\", \"Xavier\"]\n",
        "    },\n",
        "\n",
        "    \"nlh\": {\n",
        "        \"values\": [3, 4, 5]\n",
        "    },\n",
        "\n",
        "\n",
        "    \"sz\": {\n",
        "        \"values\": [32, 64, 128]\n",
        "    },\n",
        "\n",
        "    \"a\": {\n",
        "        \"values\": [ 'tanh',  'sigmoid', 'ReLU']\n",
        "    },\n",
        "\n",
        "    \"lr\": {\n",
        "        \"values\": [0.001, 0.0001]\n",
        "    },\n",
        "\n",
        "\n",
        "    \"w_d\": {\n",
        "        \"values\": [0, 0.0005,0.5]\n",
        "    },\n",
        "\n",
        "    \"o\": {\n",
        "        \"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\",\"nadam\"]\n",
        "    },\n",
        "\n",
        "    \"b\": {\n",
        "        \"values\": [16, 32, 64]\n",
        "    }\n",
        "  }\n",
        "}\n",
        "sweep_id = wandb.sweep(sweep_config,project=\"DL_Assignment_1\", entity='cs23m008')\n",
        "\n",
        "def train_data(config = None):\n",
        "  config_defaults = dict(\n",
        "          e=10,\n",
        "          nlh=2,\n",
        "          sz=128,\n",
        "          w_d=0,\n",
        "          lr=1e-3,\n",
        "          o=\"sgd\",\n",
        "          b=32,\n",
        "          a=\"sigmoid\",\n",
        "          w_i=\"Xavier\",\n",
        "          l=\"cross_entropy\",\n",
        "      )\n",
        "  wandb.init(project='DL_Assignment_1', entity='cs23m008',config = config_defaults)\n",
        "  config = wandb.config\n",
        "  wandb.run.name = 'hl_'+str(config.nlh)+'_sz_'+str(config.sz)+'_bs_'+str(config.b)+'_ac_'+config.a+'_w_i_'+config.w_i+'_lr_'+str(config.lr)+'_wd_'+str(config.w_d)\n",
        "  train(trainX=trainX,\n",
        "      trainy=trainy,\n",
        "      textX=testX,\n",
        "      testy=needed_y_test,\n",
        "      number_hidden_layers=config.nlh,\n",
        "      hidden_layer_size=config.sz,\n",
        "      eta=config.lr,\n",
        "      init_type=config.w_i,\n",
        "      activation_function=config.a,\n",
        "      epochs=config.e,\n",
        "      mini_batch_size=config.b,\n",
        "      loss_function=config.l,\n",
        "      optimizer=config.o,\n",
        "      output_function='softmax',\n",
        "      weight_decay_const=config.w_d,\n",
        "      wandb_flag=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "L3FQXM_BcKP2"
      },
      "outputs": [],
      "source": [
        "# wandb.agent(sweep_id,train_data,count=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9_Bg06CO3rd"
      },
      "source": [
        "# Question 7\n",
        "Confussion matrix code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566,
          "referenced_widgets": [
            "db4324e0437b48d9bbe9c5702a5a7e05",
            "e66c296b63a643aab368c2963c3c772f",
            "cd68d820350b45ffbc39ffc3edef3089",
            "f569aa9e9a75486290753c7edb9e90a1",
            "124c69bb33b44962939fe8f7956162ab",
            "412f88a2a61343b4b46169bbc7a1def1",
            "15bb9e0fec6d45a4a98ffa0cf3e1f495",
            "70de4dd9c00247c28114f0e0fd085583"
          ]
        },
        "id": "RLIaPP7YbppJ",
        "outputId": "7d30122e-7723-498a-c67d-116a045c4adc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m008\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240315_152107-y0sg1so3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs23m008/DL_Assignment_1/runs/y0sg1so3' target=\"_blank\">twilight-donkey-263</a></strong> to <a href='https://wandb.ai/cs23m008/DL_Assignment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs23m008/DL_Assignment_1' target=\"_blank\">https://wandb.ai/cs23m008/DL_Assignment_1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs23m008/DL_Assignment_1/runs/y0sg1so3' target=\"_blank\">https://wandb.ai/cs23m008/DL_Assignment_1/runs/y0sg1so3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch :  1  validation loss :  0.5246686119321946\n",
            "epoch :  2  validation loss :  0.4520215002912937\n",
            "epoch :  3  validation loss :  0.42133751384283513\n",
            "epoch :  4  validation loss :  0.4029768079422654\n",
            "epoch :  5  validation loss :  0.390291106887213\n",
            "epoch :  6  validation loss :  0.3807933706795419\n",
            "epoch :  7  validation loss :  0.37326590493774403\n",
            "epoch :  8  validation loss :  0.36703543116416554\n",
            "epoch :  9  validation loss :  0.3617043989651403\n",
            "epoch :  10  validation loss :  0.35702713811397296\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db4324e0437b48d9bbe9c5702a5a7e05",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.007 MB of 0.007 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁▅▆▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅▆▇▇▇▇██</td></tr><tr><td>val_loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.88246</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.32907</td></tr><tr><td>val_accuracy</td><td>0.87092</td></tr><tr><td>val_loss</td><td>0.35703</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">twilight-donkey-263</strong> at: <a href='https://wandb.ai/cs23m008/DL_Assignment_1/runs/y0sg1so3' target=\"_blank\">https://wandb.ai/cs23m008/DL_Assignment_1/runs/y0sg1so3</a><br/>Synced 5 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20240315_152107-y0sg1so3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(project='DL_Assignment_1', entity='cs23m008')\n",
        "run.name = 'Confusion Matrix'\n",
        "# test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "# print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=3,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.0001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"tanh\",\n",
        "    epochs=10,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"cross_entropy\",\n",
        "    optimizer=\"nadam\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.0005,\n",
        "    wandb_flag=True)\n",
        "a,h = forward_propagation(testX,params[0],params[1],params[2], params[3], params[4])\n",
        "hL = h[-1]\n",
        "y_pred = np.zeros(len(hL))\n",
        "i = 0\n",
        "while(i!=len(hL)):\n",
        "  y_pred[i] = np.argmax(hL[i])\n",
        "  i+=1\n",
        "y_pred\n",
        "\n",
        "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "\n",
        "wandb.log({\"The Confusion Marix\": wandb.plot.confusion_matrix(preds = y_pred,y_true=test_Y,class_names=class_names)})\n",
        "wandb.save()\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NapckCR5bZvM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puB16k6uEr1F"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(project='DL_Assignment_1', entity='cs23m008')\n",
        "run.name = 'mean square error'\n",
        "\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=4,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"tanh\",\n",
        "    epochs=10,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"mean_squared_error\",\n",
        "    optimizer=\"nadam\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.5,\n",
        "    wandb_flag=True)\n",
        "\n",
        "# test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "# print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "wandb.save()\n",
        "wandb.finish()\n",
        "\n",
        "run = wandb.init(project='DL_Assignment_1', entity='cs23m008')\n",
        "run.name = 'Cross Entropy'\n",
        "\n",
        "params = train(trainX=trainX,\n",
        "    trainy=trainy,\n",
        "    textX=testX,\n",
        "    testy=needed_y_test,\n",
        "    number_hidden_layers=4,\n",
        "    hidden_layer_size=128,\n",
        "    eta=0.001,\n",
        "    init_type=\"Xavier\",\n",
        "    activation_function=\"tanh\",\n",
        "    epochs=10,\n",
        "    mini_batch_size=32,\n",
        "    loss_function=\"cross_entropy\",\n",
        "    optimizer=\"nadam\",\n",
        "    output_function='softmax',\n",
        "    weight_decay_const=0.5,\n",
        "    wandb_flag=True)\n",
        "\n",
        "# test_ac = test_accuracy(testX,needed_y_test,params[0],params[1],params[2],params[3],params[4])\n",
        "# print(\"Test accuracy on the model = \", test_ac*100,'%')\n",
        "wandb.save()\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XxB8c1TTyWX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "124c69bb33b44962939fe8f7956162ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15bb9e0fec6d45a4a98ffa0cf3e1f495": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "412f88a2a61343b4b46169bbc7a1def1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70de4dd9c00247c28114f0e0fd085583": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd68d820350b45ffbc39ffc3edef3089": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15bb9e0fec6d45a4a98ffa0cf3e1f495",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70de4dd9c00247c28114f0e0fd085583",
            "value": 1
          }
        },
        "db4324e0437b48d9bbe9c5702a5a7e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e66c296b63a643aab368c2963c3c772f",
              "IPY_MODEL_cd68d820350b45ffbc39ffc3edef3089"
            ],
            "layout": "IPY_MODEL_f569aa9e9a75486290753c7edb9e90a1"
          }
        },
        "e66c296b63a643aab368c2963c3c772f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_124c69bb33b44962939fe8f7956162ab",
            "placeholder": "​",
            "style": "IPY_MODEL_412f88a2a61343b4b46169bbc7a1def1",
            "value": "0.018 MB of 0.018 MB uploaded\r"
          }
        },
        "f569aa9e9a75486290753c7edb9e90a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
